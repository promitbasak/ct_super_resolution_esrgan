{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74823ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "import torch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8ceb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to customize\n",
    "SLICE_DEPTH = 1          # Number of consecutive slices per input (channel size)\n",
    "SLICE_DROP = 5           # Number of slices to drop between input tensors\n",
    "INPUT_SIZE = 128         # Final image size\n",
    "OUTPUT_FOLDER = \"../processed_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "627e6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nii_and_normalize(path):\n",
    "    img = nib.load(path).get_fdata()\n",
    "    img = np.clip(img, -1000, 1000)  # Clip Hounsfield units\n",
    "    img = (img + 1000) / 2000        # Normalize to 0â€“1\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e7621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(volume, depth=5, drop=2, size=128):\n",
    "    slices = volume.shape[2]\n",
    "    outputs = []\n",
    "    idx = 0\n",
    "    while idx + depth <= slices:\n",
    "        chunk = volume[:, :, idx:idx+depth]  # Shape: H x W x depth\n",
    "        chunk = np.transpose(chunk, (2, 0, 1))  # Shape: depth x H x W\n",
    "        chunk = resize(chunk, (depth, size, size), mode='constant')\n",
    "        outputs.append(torch.tensor(chunk, dtype=torch.float32))\n",
    "        idx += depth + drop\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ecbf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(root_dir, output_dir, save_as_image=False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for split in [\"train\", \"valid\"]:\n",
    "        split_path = Path(root_dir) / split\n",
    "        for case in split_path.glob(\"*/*.nii.gz\"):\n",
    "            vol = load_nii_and_normalize(str(case))\n",
    "            patches = extract_patches(vol, SLICE_DEPTH, SLICE_DROP, INPUT_SIZE)\n",
    "\n",
    "            case_name = case.stem\n",
    "            out_path = Path(output_dir) / split / case_name\n",
    "            out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for i, patch in enumerate(patches):\n",
    "                if save_as_image:\n",
    "                    save_image(patch, open(out_path / f\"{case_name}_patch_{i}.png\", \"wb\"))\n",
    "                else:\n",
    "                    torch.save(patch, out_path / f\"{case_name}_patch_{i}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad6254ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dataset(\"../CT_rate/dataset\", \"../sample_outputs\", True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
