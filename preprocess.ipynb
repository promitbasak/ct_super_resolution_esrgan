{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74823ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage import zoom\n",
    "from skimage.transform import resize\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce4c95",
   "metadata": {},
   "source": [
    "# Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9031f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_for_volume(metadata, volume_name):\n",
    "    # Find the row where the VolumeName matches\n",
    "    volume_metadata = metadata[metadata['VolumeName'] == volume_name]\n",
    "    \n",
    "    # Return metadata as a dictionary (could include other info like spacing, rescale, etc.)\n",
    "    if not volume_metadata.empty:\n",
    "        return volume_metadata.iloc[0].to_dict()\n",
    "    else:\n",
    "        raise ValueError(f\"No metadata found for volume: {volume_name}\")\n",
    "\n",
    "def convert_to_hu(image, slope, intercept):\n",
    "    \"\"\"Convert image data to Hounsfield Units using slope and intercept.\"\"\"\n",
    "    hu_image = image * slope + intercept\n",
    "    hu_image = np.clip(hu_image, -1000, 1000)  # Clip HU to the valid range\n",
    "    return hu_image\n",
    "\n",
    "def resample_volume(volume, current_spacing, target_spacing=(0.75, 0.75, 1.5)):\n",
    "    \"\"\"Resample the volume to the desired spacing using zoom.\"\"\"\n",
    "    zoom_factors = [curr / tgt for curr, tgt in zip(current_spacing, target_spacing)]\n",
    "    resampled = zoom(volume, zoom=zoom_factors, order=1)  # Linear interpolation\n",
    "    return resampled\n",
    "\n",
    "def crop_or_pad(volume, target_shape=(480, 480, 240)):\n",
    "    \"\"\"Center crop or pad the volume to the target shape.\"\"\"\n",
    "    output = np.zeros(target_shape, dtype=np.float32)\n",
    "    min_shape = np.minimum(volume.shape, target_shape)\n",
    "    start_src = [(s - m) // 2 for s, m in zip(volume.shape, min_shape)]\n",
    "    start_dst = [(t - m) // 2 for t, m in zip(target_shape, min_shape)]\n",
    "    \n",
    "    output[\n",
    "        start_dst[0]:start_dst[0] + min_shape[0],\n",
    "        start_dst[1]:start_dst[1] + min_shape[1],\n",
    "        start_dst[2]:start_dst[2] + min_shape[2]\n",
    "    ] = volume[\n",
    "        start_src[0]:start_src[0] + min_shape[0],\n",
    "        start_src[1]:start_src[1] + min_shape[1],\n",
    "        start_src[2]:start_src[2] + min_shape[2]\n",
    "    ]\n",
    "    return output\n",
    "\n",
    "def preprocess_ct_rate_volume(nii_path, metadata, output_path, target_spacing=(0.75, 0.75, 1.5), target_shape=(512, 512, 240)):\n",
    "    \"\"\"Preprocess the CT volume by using metadata from the CSV.\"\"\"\n",
    "    \n",
    "    # Extract the volume name (without extension)\n",
    "    volume_name = Path(nii_path).name\n",
    "    \n",
    "    # Get metadata for the volume from the CSV\n",
    "    volume_metadata = get_metadata_for_volume(metadata, volume_name)\n",
    "\n",
    "    # Load the NIfTI image\n",
    "    img = nib.load(nii_path)\n",
    "    volume = img.get_fdata()\n",
    "    affine = img.affine\n",
    "\n",
    "    # Use metadata for RescaleSlope and RescaleIntercept\n",
    "    rescale_intercept = volume_metadata['RescaleIntercept']\n",
    "    rescale_slope = volume_metadata['RescaleSlope']\n",
    "    \n",
    "    # 1. Convert to HU using metadata\n",
    "    volume = convert_to_hu(volume, rescale_slope, rescale_intercept)\n",
    "\n",
    "    # 2. Resample volume to target spacing\n",
    "    spacing = ast.literal_eval(volume_metadata[\"XYSpacing\"]) + [int(volume_metadata[\"ZSpacing\"])]\n",
    "    volume = resample_volume(volume, spacing[::-1], target_spacing=target_spacing)\n",
    "\n",
    "    # 3. Crop or pad volume to target shape\n",
    "    volume = crop_or_pad(volume, target_shape=target_shape)\n",
    "\n",
    "    # 4. Save the preprocessed volume as .nii.gz\n",
    "    new_img = nib.Nifti1Image(volume.astype(np.float32), affine)\n",
    "    output_path = Path(output_path)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    nib.save(new_img, output_path.with_suffix(\".nii.gz\"))\n",
    "\n",
    "\n",
    "def batch_preprocess_ct_rate(input_root=\"dataset\", output_root=\"preprocessed\"):\n",
    "    input_root = Path(input_root)\n",
    "    output_root = Path(output_root)\n",
    "\n",
    "    for split in [\"train\", \"valid\"]:\n",
    "        split_dir = input_root / split\n",
    "        for case_folder in split_dir.iterdir():\n",
    "            if case_folder.is_dir():\n",
    "                for nii_file in case_folder.glob(\"*.nii.gz\"):\n",
    "                    out_file = output_root / split / case_folder.name / nii_file.stem\n",
    "                    preprocess_ct_rate_volume(nii_file, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ad8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nii_and_normalize(path):\n",
    "    img = nib.load(path).get_fdata()\n",
    "    img = np.clip(img, -1000, 1000)  # Clip Hounsfield units\n",
    "    img = (img + 1000) / 2000        # Normalize to 0â€“1\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patches(volume, depth=5, drop=2, size=128):\n",
    "    slices = volume.shape[2]\n",
    "    outputs = []\n",
    "    idx = 0\n",
    "    while idx + depth <= slices:\n",
    "        chunk = volume[:, :, idx:idx+depth]  # Shape: H x W x depth\n",
    "        chunk = np.transpose(chunk, (2, 0, 1))  # Shape: depth x H x W\n",
    "        chunk = resize(chunk, (depth, size, size), mode=\"constant\")\n",
    "        outputs.append(torch.tensor(chunk, dtype=torch.float32))\n",
    "        idx += depth + drop\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67545d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(root_dir, output_dir, slice_depth, slice_drop, input_size, save_as_image=False):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for split in [\"train\", \"valid\"]:\n",
    "        split_path = Path(root_dir) / split\n",
    "        for case in split_path.glob(\"*/*.nii.gz\"):\n",
    "            vol = load_nii_and_normalize(str(case))\n",
    "            patches = extract_patches(vol, slice_depth, slice_drop, input_size)\n",
    "\n",
    "            case_name = case.stem\n",
    "            out_path = Path(output_dir) / split / case_name\n",
    "            out_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            for i, patch in enumerate(patches):\n",
    "                if save_as_image:\n",
    "                    save_image(patch, open(out_path / f\"{case_name}_patch_{i}.png\", \"wb\"))\n",
    "                else:\n",
    "                    torch.save(patch, out_path / f\"{case_name}_patch_{i}.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc28482",
   "metadata": {},
   "source": [
    "# Process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ceb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to customize\n",
    "SLICE_DEPTH = 1          # Number of consecutive slices per input (channel size)\n",
    "SLICE_DROP = 5           # Number of slices to drop between input tensors\n",
    "INPUT_SIZE = 128         # Final image size\n",
    "OUTPUT_FOLDER = \"../processed_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6254ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dataset(\"../CT_rate/dataset/\", \"../sample_outputs\", SLICE_DEPTH, SLICE_DROP, INPUT_SIZE, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edfac2f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_metadata_for_volume' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Use metadata for RescaleSlope and RescaleIntercept\u001b[39;00m\n\u001b[32m     11\u001b[39m metadata = pd.read_csv(metadata_path)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m volume_metadata = \u001b[43mget_metadata_for_volume\u001b[49m(metadata, nii_path.name)\n\u001b[32m     13\u001b[39m rescale_intercept = volume_metadata[\u001b[33m'\u001b[39m\u001b[33mRescaleIntercept\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     14\u001b[39m rescale_slope = volume_metadata[\u001b[33m'\u001b[39m\u001b[33mRescaleSlope\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'get_metadata_for_volume' is not defined"
     ]
    }
   ],
   "source": [
    "nii_path = Path(\"../CT_RATE/dataset/train/train_1_a/train_1_a_1.nii.gz\")\n",
    "metadata_path = Path(\"../CT_RATE/dataset/train_metadata.csv\")\n",
    "target_spacing = (0.75, 0.75, 1)\n",
    "target_shape = (512, 512, 240)\n",
    "\n",
    "img = nib.load(nii_path)\n",
    "volume = img.get_fdata()\n",
    "affine = img.affine\n",
    "\n",
    "# Use metadata for RescaleSlope and RescaleIntercept\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "volume_metadata = get_metadata_for_volume(metadata, nii_path.name)\n",
    "rescale_intercept = volume_metadata['RescaleIntercept']\n",
    "rescale_slope = volume_metadata['RescaleSlope']\n",
    "\n",
    "# 1. Convert to HU using metadata\n",
    "volume = convert_to_hu(volume, rescale_slope, rescale_intercept)\n",
    "\n",
    "# 2. Resample volume to target spacing\n",
    "spacing = ast.literal_eval(volume_metadata[\"XYSpacing\"]) + [int(volume_metadata[\"ZSpacing\"])]\n",
    "volume = resample_volume(volume, spacing[::-1], target_spacing=target_spacing)\n",
    "\n",
    "# 3. Crop or pad volume to target shape\n",
    "volume = crop_or_pad(volume, target_shape=target_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1facac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "429cc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_path = \"../CT_RATE/dataset/train/train_1_a/train_1_a_1.nii.gz\"\n",
    "output_path = \"../CT_RATE/dataset/train/train_1_a/train_1_a_1_reconstructed2\"\n",
    "metadata_path = \"../CT_RATE/dataset/train_metadata.csv\"\n",
    "\n",
    "preprocess_ct_rate_volume(\n",
    "    nii_path,\n",
    "    metadata,\n",
    "    output_path,\n",
    "    (1, 1, 1.5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f23202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
